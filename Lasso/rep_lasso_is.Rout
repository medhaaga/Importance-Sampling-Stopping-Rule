
R version 4.2.1 (2022-06-23) -- "Funny-Looking Kid"
Copyright (C) 2022 The R Foundation for Statistical Computing
Platform: x86_64-apple-darwin17.0 (64-bit)

R is free software and comes with ABSOLUTELY NO WARRANTY.
You are welcome to redistribute it under certain conditions.
Type 'license()' or 'licence()' for distribution details.

  Natural language support but running in an English locale

R is a collaborative project with many contributors.
Type 'contributors()' for more information and
'citation()' on how to cite R or R packages in publications.

Type 'demo()' for some demos, 'help()' for on-line help, or
'help.start()' for an HTML browser interface to help.
Type 'q()' to quit R.

> # loading required packages
> library(INLA)
Loading required package: Matrix
Loading required package: foreach
Loading required package: parallel
Loading required package: sp
This is INLA_22.05.07 built 2022-05-07 09:58:26 UTC.
 - See www.r-inla.org/contact-us for how to get help.
 - To enable PARDISO sparse library; see inla.pardiso()
> library(ISLR)
> library(glmnet)
Loaded glmnet 4.1-4
> library(smoothmest)
Loading required package: MASS
> library(mvtnorm)
> 
> # sourcing INLA-IS, INLA-AMIS and INLA-MH code
> source("inlaIS_reps.R")
> source("genFuncs.R")
> load("Out/lasso_is_init.Rdata")
> 
> data(Hitters)
> 
> #Check NA's and fix
> 
> Hitters <- na.omit(Hitters)
> 
> #
> # The Lasso
> #
> 
> #Create variables for lasso
> x <- model.matrix(Salary ~ ., Hitters)[, -1]
> x <- x[, 1:5] #Just for testing
> x <- scale(x)
> y <- Hitters$Salary
> y <- scale(y)
> df <- list(y = y, x = x)
> n.beta <- ncol(df$x)
> 
> # ml estimates
> ml = summary(lm(y~-1 + x, data = df))$coefficients[,1:2]
> 
> #Indices for train/test model
> set.seed(1)
> train <- sample(1:nrow(x), nrow(x)/2)
> test <- (-train)
> 
> #Grid for lambda parameter in lasso
> grid <- 10^seq(10, -2, length = 100)
> 
> #Fit lasso model for several values of lambda
> lasso.mod <- glmnet(x[train, ] , y[train], alpha = 1, lambda = grid,intercept = F)
> 
> #CV
> set.seed(1)
> cv.out <- cv.glmnet(x[train, ], y[train], alpha = 1,intercept=F)
> 
> #Take best lambda for lasso model
> bestlam <- cv.out$lambda.min
> 
> #Predcit with lasso on test data
> lasso.pred <- predict(lasso.mod, s = bestlam, newx = x[test, ])
> 
> #Fit model to complete dataset
> out <- glmnet(x, y, alpha = 1, lambda = grid,intercept=F)
> lasso.coef <- predict(out, type = "coefficients", s = bestlam)
> 
> 
> #Fitted values
> lasso.fitted <- predict(out, s = bestlam, newx = x)
> # importing dataset
> 
> # finding inverse of the precision
> stdev.samp <- .25 * solve(t(x)%*%x)
> 
> 
> fit.inla <- function(data, eta) {
+   data$oset = data$x %*% eta
+   res = inla(y ~ -1 + offset(oset), data = data)
+   res = inla.rerun(res)
+   return(list(mlik = res$mlik[[1]],
+               dists = list(tau = res$marginals.hyperpar[[1]]),
+               stats = list(tau = as.numeric(res$summary.hyperpar[1]))))
+ }
> 
> 
> 
> prior.beta <- function(x, mu = 0, lambda = 0.073, log = TRUE) {
+   res <- sum(log(ddoublex(x, mu = mu, lambda = lambda)))
+   
+   if(!log) { res <- exp(res) }
+   
+   return(res)
+ }
> 
> 
> # proposal distribution
> ## evaluate
> dq.beta <- function(y, theta = init, log =TRUE) {
+   #dmvnorm(y,mean = x, sigma = sigma,log = log)
+   dmvt(y,delta=theta[[1]],sigma=theta[[2]],df=3,log=log,type = "shifted")
+ }
> ## sample
> rq.beta <- function(theta) {
+   #rmvnorm(1,mean=x,sigma = sigma)
+   as.vector(rmvt(1,sigma = theta[[2]], df=3, delta = theta[[1]], type = "shifted"))
+ }
> 
> # initial parameters of the proposal distribution
> init <- is_mod$theta[[length(is_mod$theta)]]
> 
> ### IS replication study
> set.seed(1)
> reps <- 10
> all_out <- list(length = reps)
> for(r in 1:reps)
+ {
+   start.time <- Sys.time()
+   all_out[[r]] = inlaIS(data = df, init = init,
+                 prior.beta, dq.beta, rq.beta, fit.inla, N = 10000,ncores = 5)
+   end.time <- Sys.time()
+   print(end.time - start.time) 
+ }
Time difference of 2.700009 hours
Time difference of 2.794814 hours
Time difference of 2.557588 hours
Time difference of 2.557188 hours
Time difference of 2.555192 hours
Time difference of 2.553138 hours
Time difference of 2.552429 hours
Time difference of 2.553985 hours
Time difference of 2.557487 hours
Time difference of 2.56299 hours
> 
> 
> save(all_out, file = "Out/lasso_reps.Rdata")
> 
> proc.time()
     user    system   elapsed 
145009.17 189781.31  93403.16 
