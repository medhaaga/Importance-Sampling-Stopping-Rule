
R version 4.2.1 (2022-06-23) -- "Funny-Looking Kid"
Copyright (C) 2022 The R Foundation for Statistical Computing
Platform: x86_64-pc-linux-gnu (64-bit)

R is free software and comes with ABSOLUTELY NO WARRANTY.
You are welcome to redistribute it under certain conditions.
Type 'license()' or 'licence()' for distribution details.

  Natural language support but running in an English locale

R is a collaborative project with many contributors.
Type 'contributors()' for more information and
'citation()' on how to cite R or R packages in publications.

Type 'demo()' for some demos, 'help()' for on-line help, or
'help.start()' for an HTML browser interface to help.
Type 'q()' to quit R.

> # loading required packages
> library(INLA)
Loading required package: Matrix
Loading required package: foreach
Loading required package: parallel
Loading required package: sp
This is INLA_22.05.07 built 2022-05-07 09:52:03 UTC.
 - See www.r-inla.org/contact-us for how to get help.
 - To enable PARDISO sparse library; see inla.pardiso()
> library(ISLR)
> library(glmnet)
Loaded glmnet 4.1-4
> library(smoothmest)
Loading required package: MASS
> library(mvtnorm)
> 
> # sourcing INLA-IS, INLA-AMIS and INLA-MH code
> source("inlaIS_reps.R")
> source("genFuncs.R")
> load("Out/lasso_is_init.Rdata")
> 
> data(Hitters)
> 
> #Check NA's and fix
> 
> Hitters <- na.omit(Hitters)
> 
> #
> # The Lasso
> #
> 
> #Create variables for lasso
> x <- model.matrix(Salary ~ ., Hitters)[, -1]
> # x <- x[, 1:5] #Just for testing
> x <- scale(x)
> y <- Hitters$Salary
> y <- scale(y)
> df <- list(y = y, x = x)
> n.beta <- ncol(df$x)
> 
> # ml estimates
> ml = summary(lm(y~-1 + x, data = df))$coefficients[,1:2]
> 
> #Indices for train/test model
> set.seed(1)
> train <- sample(1:nrow(x), nrow(x)/2)
> test <- (-train)
> 
> #Grid for lambda parameter in lasso
> grid <- 10^seq(10, -2, length = 100)
> 
> #Fit lasso model for several values of lambda
> lasso.mod <- glmnet(x[train, ] , y[train], alpha = 1, lambda = grid,intercept = F)
> 
> #CV
> set.seed(1)
> cv.out <- cv.glmnet(x[train, ], y[train], alpha = 1,intercept=F)
> 
> #Take best lambda for lasso model
> bestlam <- cv.out$lambda.min
> 
> #Predcit with lasso on test data
> lasso.pred <- predict(lasso.mod, s = bestlam, newx = x[test, ])
> 
> #Fit model to complete dataset
> out <- glmnet(x, y, alpha = 1, lambda = grid,intercept=F)
> lasso.coef <- predict(out, type = "coefficients", s = bestlam)
> 
> 
> #Fitted values
> lasso.fitted <- predict(out, s = bestlam, newx = x)
> # importing dataset
> 
> # finding inverse of the precision
> stdev.samp <- .25 * solve(t(x)%*%x)
> 
> 
> fit.inla <- function(data, eta) {
+   data$oset = data$x %*% eta
+   res = inla(y ~ -1 + offset(oset), data = data)
+   res = inla.rerun(res)
+   return(list(mlik = res$mlik[[1]],
+               dists = list(tau = res$marginals.hyperpar[[1]]),
+               stats = list(tau = as.numeric(res$summary.hyperpar[1]))))
+ }
> 
> 
> 
> prior.beta <- function(x, mu = 0, lambda = 0.073, log = TRUE) {
+   res <- sum(log(ddoublex(x, mu = mu, lambda = lambda)))
+   
+   if(!log) { res <- exp(res) }
+   
+   return(res)
+ }
> 
> 
> # proposal distribution
> ## evaluate
> dq.beta <- function(y, theta = init, log =TRUE) {
+   #dmvnorm(y,mean = x, sigma = sigma,log = log)
+   dmvt(y,delta=theta[[1]],sigma=theta[[2]],df=3,log=log,type = "shifted")
+ }
> ## sample
> rq.beta <- function(theta) {
+   #rmvnorm(1,mean=x,sigma = sigma)
+   as.vector(rmvt(1,sigma = theta[[2]], df=3, delta = theta[[1]], type = "shifted"))
+ }
> 
> # initial parameters of the proposal distribution
> init <- is_mod$theta[[length(is_mod$theta)]]
> init[[2]] <- 2*init[[2]]
> ### IS replication study
> set.seed(1)
> reps <- 10
> all_out <- list(length = reps)
> for(r in 1:reps)
+ {
+   start.time <- Sys.time()
+   all_out[[r]] = inlaIS(data = df, init = init,
+                 prior.beta, dq.beta, rq.beta, fit.inla, N = 1e4,ncores = 60)
+   end.time <- Sys.time()
+   print(end.time - start.time) 
+ }
Time difference of 22.29615 mins
Time difference of 22.37972 mins
Time difference of 22.3609 mins
Time difference of 22.37539 mins
Time difference of 22.38693 mins
Time difference of 22.31181 mins
Time difference of 22.35432 mins
Time difference of 22.32954 mins
Time difference of 22.36488 mins
Time difference of 22.31082 mins
> 
> 
> save(all_out, file = "Out/lasso_reps.Rdata")
> 
> proc.time()
     user    system   elapsed 
828839.23  23250.32  13410.36 
